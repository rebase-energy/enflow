{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import enflow as ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gefcom2014-wind', 'gefcom2014-solar-all', 'gefcom2014-solar']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.list_problems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, env, obj = tr.load_problem(\"gefcom2014-solar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio (edm.Portfolio)\n",
      "├── Site1 (edm.PVSystem)\n",
      "├── Site2 (edm.PVSystem)\n",
      "└── Site3 (edm.PVSystem)\n"
     ]
    }
   ],
   "source": [
    "portfolio = dataset.collection\n",
    "portfolio.draw_tree(show_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prediction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from contextlib import nullcontext\n",
    "\n",
    "class LGBGEFCom2014Predictor(ef.Predictor):\n",
    "    def __init__(self, portfolio: ef.Portfolio, name=None, quantiles=None):\n",
    "        \"\"\"\n",
    "        Initialize the Predictor class.\n",
    "        \n",
    "        Args:\n",
    "            quantiles (list): List of quantiles for which to create separate models.\n",
    "                              Example: [0.1, 0.5, 0.9]\n",
    "        \"\"\"\n",
    "\n",
    "        self.portfolio = portfolio\n",
    "        self.name = name\n",
    "        self.models = {}  # Dictionary to hold models for each site and quantile\n",
    "        self.quantiles = quantiles\n",
    "    \n",
    "    def preprocess(self, df_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # Reassign df to a copy only within the function scope\n",
    "        df_features = df_features.copy()\n",
    "\n",
    "        # Add lead time feature\n",
    "        ref_datetime = df_features.index.get_level_values(0)\n",
    "        valid_datetime = df_features.index.get_level_values(1)\n",
    "        lead_time = (valid_datetime-ref_datetime)/pd.Timedelta('1 hour')\n",
    "        for asset in portfolio.assets:\n",
    "            df_features.loc[:,(asset.name,'lead_time')] = lead_time\n",
    "\n",
    "        # Differentiate accumulated features\n",
    "        features_accum = ['VAR169', 'VAR175', 'VAR178', 'VAR228']\n",
    "        df_accum = df_features.loc[:,(slice(None),features_accum)]\n",
    "        df_accum = df_accum.diff()\n",
    "        df_accum[df_accum.index.get_level_values(1).hour==1] = df_features.loc[df_accum.index.get_level_values(1).hour==1,(slice(None),features_accum)]\n",
    "        df_accum.loc[:,(slice(None),features_accum[:3])] = df_accum.loc[:,(slice(None),features_accum[:3])]/3600 # Convert from J to Wh/h\n",
    "        df_features.loc[:,(slice(None),features_accum)] = df_accum \n",
    "\n",
    "        # Calculate solar position\n",
    "        index = df_features.index.get_level_values(1)\n",
    "        for asset in portfolio.assets: \n",
    "            df_solpos = asset.location.to_pvlib().get_solarposition(index)\n",
    "            df_solpos = df_solpos.loc[:, ['zenith', 'azimuth']]\n",
    "            df_solpos.index = df_features.index\n",
    "            for column in df_solpos.columns: \n",
    "                df_features[(asset.name, column)] = df_solpos[column]\n",
    "\n",
    "        # Calculate clearsky power\n",
    "        for asset in portfolio.assets: \n",
    "            clearsky_power = tr.clearsky_power(asset, index)\n",
    "            df_features[(asset.name, \"clearsky_power\")] = clearsky_power\n",
    "        \n",
    "        # Calculate physical power\n",
    "        for asset in portfolio.assets: \n",
    "            physical_power = tr.physical_power(asset, df_features[(asset.name, \"VAR169\")].droplevel(0))\n",
    "            df_features[(asset.name, \"physical_power\")] = physical_power.values \n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def train(self, df_features: pd.DataFrame, target: pd.DataFrame, show_progress=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Train separate LightGBM models for each site and quantile.\n",
    "        \n",
    "        Args:\n",
    "            features (pd.DataFrame): Multi-indexed dataframe where the top-level index corresponds to sites.\n",
    "            target (pd.DataFrame): The target dataframe (y), also multi-indexed by site.\n",
    "            kwargs: Additional parameters to pass to the LightGBMRegressor model.\n",
    "        \"\"\"\n",
    "\n",
    "        df_features = self.preprocess(df_features)\n",
    "\n",
    "        # Get the list of unique sites from the multi-index (top level)\n",
    "        sites = df_features.columns.get_level_values(0).unique()\n",
    "        feature_names = df_features.columns.get_level_values(1).unique()\n",
    "\n",
    "        pbar = tqdm(total=len(sites)*len(self.quantiles), mininterval=0, desc=f\"Training {self.name}\") if show_progress else nullcontext()\n",
    "\n",
    "        with pbar as progress: \n",
    "            # Loop over each site\n",
    "            for site in sites:\n",
    "                # Extract the features and target for the current site\n",
    "                site_features = df_features.xs(site, axis=1, level=0)\n",
    "                site_target = target.xs(site, axis=1, level=0)\n",
    "                site_target = site_target[\"Power\"]-site_features[\"physical_power\"]\n",
    "\n",
    "                # Loop over each quantile\n",
    "                for quantile in self.quantiles:\n",
    "                    # Initialize a LightGBM model for this quantile\n",
    "                    params = {'objective': 'quantile', 'alpha': quantile, \"verbose\": -1}\n",
    "                    params.update(kwargs)  # Add any additional LightGBM parameters\n",
    "                    \n",
    "                    model = lgb.LGBMRegressor(**params)\n",
    "                    \n",
    "                    # Train the model on the site's data\n",
    "                    model.fit(site_features, site_target)\n",
    "                    \n",
    "                    # Store the trained model with a key (site, quantile)\n",
    "                    self.models[(site, quantile)] = model\n",
    "\n",
    "                    if show_progress: \n",
    "                        progress.update(1)\n",
    "\n",
    "    def predict(self, df_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Make predictions for a specific site and quantile using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            df_features (pd.DataFrame): The feature dataframe (X), multi-indexed column by site.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Predictions from the model.\n",
    "        \"\"\"\n",
    "\n",
    "        df_features = self.preprocess(df_features)\n",
    "\n",
    "        # Create a nested dictionary to store predictions\n",
    "        predictions = {}\n",
    "\n",
    "        # Extract the features for the specific site\n",
    "        sites = df_features.columns.get_level_values(0).unique()\n",
    "\n",
    "        # Loop over each site and quantile\n",
    "        for site in sites:\n",
    "            # Extract the features for the current site\n",
    "            site_features = df_features.xs(site, axis=1, level=0)\n",
    "\n",
    "            # Initialize an inner dictionary for each site\n",
    "\n",
    "            for quantile in self.quantiles:\n",
    "                # Check if the model for the given site and quantile exists\n",
    "                if (site, quantile) not in self.models:\n",
    "                    raise ValueError(f\"No trained model for site '{site}' and quantile '{quantile}'.\")\n",
    "\n",
    "                # Make predictions using the stored model\n",
    "                model = self.models[(site, quantile)]\n",
    "                site_predictions = model.predict(site_features)                      \n",
    "                site_predictions = site_predictions+site_features[\"physical_power\"].values\n",
    "\n",
    "                # Store the predictions under the quantile for the current site\n",
    "                predictions[(site, f\"quantile_{round(100*quantile)}\")] = site_predictions\n",
    "\n",
    "        # Convert the nested dictionary to a DataFrame with multi-index columns\n",
    "        df_predictions = pd.DataFrame.from_dict(predictions)\n",
    "        df_predictions.index = df_features.index\n",
    "        \n",
    "        return df_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6) Run the sequential decision loop and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data, next_input = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = initial_data[\"input\"]\n",
    "training_target = initial_data[\"target\"]\n",
    "predictor1 = LGBGEFCom2014Predictor(portfolio=portfolio, name=\"predictor1\", quantiles=[0.1, 0.5, 0.9])\n",
    "predictor1.train(df_features=training_input, target=training_target)\n",
    "\n",
    "predictor2 = predictor1.copy(name=\"predictor2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {predictor1.name: [], predictor2.name: []}\n",
    "predictions = []\n",
    "for i in range(env.n_steps):\n",
    "    prediction1 = predictor1.predict(df_features=next_input)\n",
    "    prediction2 = predictor2.predict(df_features=next_input)\n",
    "    predictions.append(prediction1)\n",
    "\n",
    "    training_input = pd.concat([training_input, next_input])\n",
    "    next_input, next_target, done = env.step()\n",
    "    training_target = pd.concat([training_target, next_target])\n",
    "\n",
    "    loss1 = obj.calculate(next_target, prediction1)\n",
    "    loss2 = obj.calculate(next_target, prediction2)\n",
    "\n",
    "    losses[predictor1.name].append(loss1)\n",
    "    losses[predictor2.name].append(loss2)\n",
    "\n",
    "    predictor2.train(df_features=training_input, target=training_target)\n",
    "\n",
    "    print(f\"{predictor1.name} {obj.name} for step {i+1}: {loss1}\")\n",
    "    print(f\"{predictor2.name} {obj.name} for step {i+1}: {loss2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.concat(predictions)\n",
    "env.plot_forecasts(training_target, df_predictions, site=\"Site1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_overall_results(losses, \n",
    "                         drop_tasks=[\"Task1\", \"Task2\", \"Task3\", \"Task4\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results(losses, \n",
    "                 drop_tasks=[\"Task1\", \"Task2\", \"Task3\", \"Task4\"], \n",
    "                 n_top_teams=3,\n",
    "                 xlim=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
